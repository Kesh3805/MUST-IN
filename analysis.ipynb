{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e0898f7",
   "metadata": {},
   "source": [
    "# MUST-IN: Multilingual Hate Speech Detection Analysis\n",
    "\n",
    "This notebook provides comprehensive analysis and visualization of the MUST-IN framework results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d2dfb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51176fe8",
   "metadata": {},
   "source": [
    "## 1. Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b209d22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw dataset\n",
    "df_raw = pd.read_csv('data/raw/sample_dataset.csv')\n",
    "print(f\"Total records: {len(df_raw)}\")\n",
    "print(f\"\\nColumns: {df_raw.columns.tolist()}\")\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3062ead6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset statistics\n",
    "print(\"Dataset Statistics:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nLabel Distribution:\")\n",
    "print(df_raw['label'].value_counts())\n",
    "print(f\"\\nLanguage Distribution:\")\n",
    "print(df_raw['language'].value_counts())\n",
    "print(f\"\\nPlatform Distribution:\")\n",
    "print(df_raw['platform'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fece8a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize label distribution\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Label distribution\n",
    "df_raw['label'].value_counts().plot(kind='bar', ax=axes[0], color='skyblue')\n",
    "axes[0].set_title('Label Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Label')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Language distribution\n",
    "df_raw['language'].value_counts().plot(kind='bar', ax=axes[1], color='lightcoral')\n",
    "axes[1].set_title('Language Distribution', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Language')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Platform distribution\n",
    "df_raw['platform'].value_counts().plot(kind='bar', ax=axes[2], color='lightgreen')\n",
    "axes[2].set_title('Platform Distribution', fontsize=14, fontweight='bold')\n",
    "axes[2].set_xlabel('Platform')\n",
    "axes[2].set_ylabel('Count')\n",
    "axes[2].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/data_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8044e6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-tabulation: Language vs Label\n",
    "crosstab = pd.crosstab(df_raw['language'], df_raw['label'])\n",
    "print(\"\\nLanguage vs Label Distribution:\")\n",
    "print(crosstab)\n",
    "\n",
    "# Visualize heatmap\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(crosstab, annot=True, fmt='d', cmap='YlOrRd', cbar_kws={'label': 'Count'})\n",
    "plt.title('Language vs Label Heatmap', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Language')\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/language_label_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba173e2",
   "metadata": {},
   "source": [
    "## 2. Text Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f05581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text length analysis\n",
    "df_raw['text_length'] = df_raw['text'].str.len()\n",
    "df_raw['word_count'] = df_raw['text'].str.split().str.len()\n",
    "\n",
    "print(\"Text Length Statistics:\")\n",
    "print(df_raw.groupby('label')[['text_length', 'word_count']].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfeb441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize text length by label\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Character length\n",
    "df_raw.boxplot(column='text_length', by='label', ax=axes[0])\n",
    "axes[0].set_title('Text Length by Label', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Label')\n",
    "axes[0].set_ylabel('Character Length')\n",
    "plt.sca(axes[0])\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Word count\n",
    "df_raw.boxplot(column='word_count', by='label', ax=axes[1])\n",
    "axes[1].set_title('Word Count by Label', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Label')\n",
    "axes[1].set_ylabel('Word Count')\n",
    "plt.sca(axes[1])\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/text_length_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9d9fbc",
   "metadata": {},
   "source": [
    "## 3. Preprocessing Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58729e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cleaned dataset\n",
    "df_clean = pd.read_csv('data/processed/dataset_cleaned.csv')\n",
    "\n",
    "# Compare original vs cleaned\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Original': df_raw['text'].head(10),\n",
    "    'Cleaned': df_clean['clean_text'].head(10)\n",
    "})\n",
    "print(\"\\nOriginal vs Cleaned Text (First 10 samples):\")\n",
    "print(\"=\" * 80)\n",
    "for idx, row in comparison_df.iterrows():\n",
    "    print(f\"\\n{idx+1}.\")\n",
    "    print(f\"  Original: {row['Original']}\")\n",
    "    print(f\"  Cleaned:  {row['Cleaned']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280f8a15",
   "metadata": {},
   "source": [
    "## 4. Model Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544af86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: This section requires running main.py first to generate confusion matrices\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "confusion_matrices = [f for f in os.listdir('results') if f.endswith('_confusion_matrix.png')]\n",
    "\n",
    "if len(confusion_matrices) > 0:\n",
    "    print(f\"Found {len(confusion_matrices)} confusion matrix visualizations\")\n",
    "    \n",
    "    # Display a few confusion matrices\n",
    "    num_to_display = min(4, len(confusion_matrices))\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, cm_file in enumerate(confusion_matrices[:num_to_display]):\n",
    "        img = Image.open(f'results/{cm_file}')\n",
    "        axes[i].imshow(img)\n",
    "        axes[i].axis('off')\n",
    "        axes[i].set_title(cm_file.replace('_confusion_matrix.png', ''), fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('results/confusion_matrices_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No confusion matrices found. Please run main.py first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07759e1a",
   "metadata": {},
   "source": [
    "## 5. Model Metrics Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d13f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary table of model performances\n",
    "# Note: These values would ideally be loaded from saved results\n",
    "# For now, this is a template for displaying results\n",
    "\n",
    "models_summary = {\n",
    "    'Model': ['MNB_Count', 'MNB_TFIDF', 'GNB_Count', 'GNB_TFIDF', \n",
    "              'SVM_Count', 'SVM_TFIDF', 'RF_Count', 'RF_TFIDF'],\n",
    "    'Vectorizer': ['Count', 'TF-IDF', 'Count', 'TF-IDF', \n",
    "                   'Count', 'TF-IDF', 'Count', 'TF-IDF'],\n",
    "    'Classifier': ['Multinomial NB', 'Multinomial NB', 'Gaussian NB', 'Gaussian NB',\n",
    "                   'SVM (Linear)', 'SVM (Linear)', 'Random Forest', 'Random Forest']\n",
    "}\n",
    "\n",
    "results_df = pd.DataFrame(models_summary)\n",
    "print(\"\\nModel Configurations:\")\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559c720f",
   "metadata": {},
   "source": [
    "## 6. LIME Explainability Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95753e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if LIME explanation exists\n",
    "lime_file = 'results/lime_explanation_traditional.html'\n",
    "if os.path.exists(lime_file):\n",
    "    print(f\"LIME explanation available at: {lime_file}\")\n",
    "    print(\"\\nOpen this file in a web browser to view the explanation.\")\n",
    "    print(\"\\nLIME provides:\")\n",
    "    print(\"  - Feature importance for the prediction\")\n",
    "    print(\"  - Words that contributed most to the classification\")\n",
    "    print(\"  - Probability scores for each class\")\n",
    "else:\n",
    "    print(\"LIME explanation not found. Please run main.py first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc2bcbd",
   "metadata": {},
   "source": [
    "## 7. Recommendations and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2c75c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PROJECT COMPLETION RECOMMENDATIONS\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\n1. DATA ENHANCEMENT:\")\n",
    "print(\"   - Collect more diverse hate speech examples\")\n",
    "print(\"   - Balance the dataset across all three labels\")\n",
    "print(\"   - Include more Romanized script variations\")\n",
    "\n",
    "print(\"\\n2. MODEL IMPROVEMENTS:\")\n",
    "print(\"   - Enable deep learning models (set RUN_DL=True in main.py)\")\n",
    "print(\"   - Experiment with XLM-RoBERTa for better multilingual support\")\n",
    "print(\"   - Implement cross-validation for robust evaluation\")\n",
    "\n",
    "print(\"\\n3. PREPROCESSING ENHANCEMENTS:\")\n",
    "print(\"   - Expand transliteration dictionaries\")\n",
    "print(\"   - Add language-specific stopword removal\")\n",
    "print(\"   - Implement advanced emoji handling\")\n",
    "\n",
    "print(\"\\n4. EVALUATION EXTENSIONS:\")\n",
    "print(\"   - Add per-language performance metrics\")\n",
    "print(\"   - Implement error analysis pipeline\")\n",
    "print(\"   - Create comparative visualization dashboard\")\n",
    "\n",
    "print(\"\\n5. DEPLOYMENT CONSIDERATIONS:\")\n",
    "print(\"   - Create REST API for model serving\")\n",
    "print(\"   - Implement model versioning\")\n",
    "print(\"   - Add monitoring and logging capabilities\")\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
